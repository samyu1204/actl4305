deviance_results[i] <- -2 * logLik(final_model)
} else {
deviance_results[i] <- NA  # Set to NA if the model is invalid
}
}
# Identify the optimal var.power value that minimizes deviance
optimal_var_power <- var_powers[which.min(na.omit(deviance_results))]
cat("Optimal var.power:", optimal_var_power, "\n")
# Fit the final Tweedie model using the optimal var.power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
var_powers <- seq(0, 2, by = 0.1)
mse_results <- numeric(length(var_powers))
train_control <- trainControl(method = "cv", number = 5)
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
model <- train(
claim_freq ~ ., data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"
)
mse_results[i] <- mean(model$resample$RMSE^2)
}
optimal_var_power <- var_powers[which.min(mse_results)]
optimal_var_power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
summary(tweedie_freq_model)
#Perfomance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model,  newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
training_MSE_tweedie <- mean((tweedie_freq_model_training_predictions-freq_training_val$claim_freq)^2)
#Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model,  newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
test_MSE_tweedie <- mean((tweedie_freq_model_prediction-freq_test$claim_freq)^2)
test_MSE_tweedie
summary(freq_training_val)
summary(tweedie_freq_model)
vars.to.remove <- c("exposure_id", "pet_gender", "pet_de_sexed_age", "pet_is_switcher", "nb_address_type_adj", "nb_suburb", "nb_state", "person_dob", "owner_age_years", "nb_breed_type",
"nb_breed_trait", "nb_breed_name_unique", "nb_breed_name_unique_concat", "exposure_id_1", "earned_units", "Total_Earned", "claim_nb", "Total_claim_amount",
"Total_claim_paid", "severity", "frequency", "is_multi_plan", "quote_time_group", "sa2_code", "nb_postcode", "is_multi_pet_plan", "pet_age_year")
frequency.model.data <- combined_data[,-which(colnames(combined_data) %in% vars.to.remove)]
frequency.model.data$qi <- as.factor(frequency.model.data$qi)
frequency.model.data <- na.omit(frequency.model.data)
str(frequency.model.data)
#Splitting into test and training
set.seed(2131)
freq_training_val_index <- sample(1:nrow(frequency.model.data), 0.7*nrow(frequency.model.data))
freq_training_val <- frequency.model.data[freq_training_val_index, ]
freq_test <- frequency.model.data[-freq_training_val_index, ]
######random forest model
freq_rf <- randomForest(claim_freq ~., data = freq_training_val, ntree = 100, importance = TRUE)
#Training fit
rf_training_pridictions <- predict(freq_rf, newdata = freq_training_val)
rf_training_pridictions <- as.vector(rf_training_pridictions)
training_mse_rf <- mean((rf_training_pridictions - freq_training_val$claim_freq)^2)
#Variable Importance
importance_values <- importance(freq_rf)
mse_importance <- importance_values[, "%IncMSE"]
mse_df <- data.frame(Variable = rownames(importance_values), MSE = mse_importance)
ggplot(mse_df, aes(x = reorder(Variable, MSE), y = MSE)) +
geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() +  # Flips the axes for better readability
labs(title = "Variable Importance (Mean Decrease MSE)",
x = "Variables",
y = "Mean Decrease MSE") +
theme_minimal()
#Test Performance
freq_rf_prediction <- predict(freq_rf, newdata = freq_test)
freq_rf_prediction <- as.vector(freq_rf_prediction)
freq_rf_prediction[freq_rf_prediction < 0] <- 0 #setting negative predictions to 0
freq_rf_test_mse <- mean((freq_rf_prediction - freq_test$claim_freq)^2)
#########Full Linear Regression########
full_frequency_LR <- lm(claim_freq~., data = freq_training_val) #training
#Summary and Training Performace
full_frequency_LR_summary <- summary(full_frequency_LR)
assumption_summaryplot <- plot(full_frequency_LR)
full_frequency_LR_summary$adj.r.squared
full_frequency_LR_summary$fstatistic
full_freq_AIC <- AIC(full_frequency_LR)
#Test Performance
full_freq_predicted_values <- predict(full_frequency_LR, newdata = freq_test, type = "response")
full_freq_predicted_values <- as.vector(full_freq_predicted_values)
full_freq_test_MSE <- mean((full_freq_predicted_values - freq_test$claim_freq)^2)
####Step wise Linear Regression####
stepwise_model_freq <- stepAIC(full_frequency_LR, direction = "both")
stepwise_model_freq_summary <- summary(stepwise_model_freq)
stepwise_model_freq_summary$adj.r.squared
predictions_stepwise <- predict(stepwise_model_freq, newdata = freq_training_val)
stepwise_model_freq_training_MSE <- mean((freq_training_val$claim_freq - predictions_stepwise)^2)
AIC(stepwise_model_freq)
#test performance
stepwise_model_freq_predicted_values <- predict(stepwise_model_freq, newdata = freq_test, type = "response")
stepwise_model_freq_predicted_values <- as.vector(stepwise_model_freq_predicted_values)
stepwise_freq_model_test_MSE <- mean((stepwise_model_freq_predicted_values-freq_test$claim_freq)^2)
####Lasso Model####
X.training <- model.matrix(claim_freq~., data = freq_training_val)
Y.training <- freq_training_val$claim_freq
freq_lasso_model <- cv.glmnet(X.training, Y.training, alpha = 1)
lasso_optimal_lambda <- freq_lasso_model$lambda.min
final_freq_lasso_model <- glmnet(X.training, Y.training, lambda = lasso_optimal_lambda, alpha = 1)
coef(final_freq_lasso_model)
lasso_freq_predictions_training <- predict(final_freq_lasso_model, newx = X.training, type = "response")
lasso_freq_predictions_training <- as.vector(lasso_freq_predictions_training)
training_lasso_MSE <- mean((lasso_freq_predictions_training-Y.training)^2)
#Test Performance
x.test <- model.matrix(claim_freq~., data = freq_test)
y.test <- freq_test$claim_freq
lasso_prediction_freq <- predict(final_freq_lasso_model, newx = x.test, type = "response")
lasso_prediction_freq <- as.vector(lasso_prediction_freq)
lasso_test_mse <- mean((lasso_prediction_freq - y.test)^2)
###Ridge###
freq_ridge_model <- cv.glmnet(X.training, Y.training, alpha = 0)
ridge_optimal_lambda <- freq_ridge_model$lambda.min
final_ridge_model_freq <- glmnet(X.training, Y.training, lambda = ridge_optimal_lambda, alpha = 0)
ridge_freq_predictions_training <- predict(final_ridge_model_freq, newx = X.training, type = "response")
ridge_freq_predictions_training <- as.vector(ridge_freq_predictions_training)
training_ridge_MSE <- mean((ridge_freq_predictions_training-Y.training)^2)
coef(final_ridge_model_freq)
#test performance#
ridge_freq_predictions <- predict(final_ridge_model_freq, newx = x.test, type = "response")
ridge_freq_predictions <- as.vector(ridge_freq_predictions)
ridge_test_mse <- mean((ridge_freq_predictions - y.test)^2)
###Elastic Net###
set.seed(12313)
alpha_values <- seq(0, 1, by = 0.1)
results <- data.frame(alpha = numeric(), lambda.min = numeric(), mse = numeric())
for (alpha in alpha_values) {
cv_fit <- cv.glmnet(X.training, Y.training, alpha = alpha, nfolds = 10)
results <- rbind(results, data.frame(alpha = alpha,
lambda.min = cv_fit$lambda.min,
mse = min(cv_fit$cvm)))
}
best_alpha <- results[which.min(results$mse), "alpha"]
best_lambda <- results[which.min(results$mse), "lambda.min"]
final_elastic_net_model <- glmnet(X.training, Y.training, alpha = best_alpha, lambda = best_lambda)
coef(final_elastic_net_model)
summary(final_elastic_net_model)
final_elastic_net_model_training_predictions <- predict(final_elastic_net_model, newx = X.training, type = "response")
final_elastic_net_model_training_predictions <- as.vector(final_elastic_net_model_training_predictions)
elastic_training_MSE <- mean((final_elastic_net_model_training_predictions-Y.training)^2)
elastic_net_cve <- min(results$mse)
##test performance
elastic_freq_predictions <- predict(final_elastic_net_model, newx = x.test, type = "response")
elastic_freq_predictions <- as.vector(elastic_freq_predictions)
elastic_test_mse <- mean((elastic_freq_predictions - y.test)^2)
#Assessing Distribution of claim_freq
mean_claim <- mean(frequency.model.data$claim_freq, na.rm = TRUE)
sd_claim <- sd(frequency.model.data$claim_freq, na.rm = TRUE)
ggplot(frequency.model.data, aes(x = claim_freq)) +
geom_density(fill = "lightblue", color = "darkblue", alpha = 0.6) +
stat_function(fun = dnorm, args = list(mean = mean_claim, sd = sd_claim),
color = "red", linetype = "dashed", size = 1) +
labs(title = "Density Plot of Claim Frequency with Normal Curve",
x = "Claim Frequency",
y = "Density") +
theme_minimal() +
xlim(0, 1)
###tweedie GLM####
var_powers <- seq(0, 2, by = 0.1)
mse_results <- numeric(length(var_powers))
train_control <- trainControl(method = "cv", number = 5)
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
model <- train(
claim_freq ~ ., data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"
)
mse_results[i] <- mean(model$resample$RMSE^2)
}
optimal_var_power <- var_powers[which.min(mse_results)]
optimal_var_power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
summary(tweedie_freq_model)
#Perfomance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model,  newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
training_MSE_tweedie <- mean((tweedie_freq_model_training_predictions-freq_training_val$claim_freq)^2)
#Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model,  newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
test_MSE_tweedie <- mean((tweedie_freq_model_prediction-freq_test$claim_freq)^2)
summary(tweedie_freq_model)
AIC(tweedie_freq_model)
optimal_var_power
var_powers <- seq(1, 2, by = 0.1)
mse_results <- numeric(length(var_powers))
train_control <- trainControl(method = "cv", number = 5)
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
model <- train(
claim_freq ~ ., data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"
)
mse_results[i] <- mean(model$resample$RMSE^2)
}
optimal_var_power <- var_powers[which.min(mse_results)]
optimal_var_power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
summary(tweedie_freq_model)
#Perfomance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model,  newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
training_MSE_tweedie <- mean((tweedie_freq_model_training_predictions-freq_training_val$claim_freq)^2)
#Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model,  newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
test_MSE_tweedie <- mean((tweedie_freq_model_prediction-freq_test$claim_freq)^2)
summary(tweedie_freq_model)
test_MSE_tweedie
summary(tweedie_freq_model)
tweedie_freq_model$deviance
var_powers <- seq(1, 2, by = 0.1)
# Initialize a vector to store the deviance results
deviance_results <- numeric(length(var_powers))
# Set up cross-validation control
train_control <- trainControl(method = "cv", number = 5, returnResamp = "final")
# Loop through each var.power value
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
# Train the Tweedie model with the current var.power
model <- train(
claim_freq ~ .,
data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"  # We still use RMSE to evaluate, but we will store deviance below
)
# Calculate the deviance for the model
# Note: The 'deviance' function works directly on glm objects
deviance_results[i] <- model$results$Deviance
}
var_powers <- seq(1, 2, by = 0.1)
# Initialize a vector to store the deviance results
deviance_results <- numeric(length(var_powers))
# Set up cross-validation control
train_control <- trainControl(method = "cv", number = 5, returnResamp = "final")
# Loop through each var.power value
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
# Train the Tweedie model with the current var.power
model <- train(
claim_freq ~ .,
data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"  # We still use RMSE to evaluate, but we will store deviance below
)
# Calculate the deviance for the model
# The fitted model can be extracted from train() results
# Note: The fitted model may not be directly accessible. Therefore, use the following approach.
# Manually calculate the deviance using the model
fitted_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = current_power, link = "log"))
# Store the deviance
deviance_results[i] <- deviance(fitted_model)
}
# Identify the optimal var.power value that minimizes deviance
optimal_var_power <- var_powers[which.min(deviance_results)]
cat("Optimal var.power:", optimal_var_power, "\n")
# Fit the final Tweedie model using the optimal var.power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
# Summarize the final model
summary(tweedie_freq_model)
# Performance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model, newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
# Calculate training deviance
training_deviance_tweedie <- deviance(tweedie_freq_model)
# Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model, newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
# Calculate test deviance
test_deviance_tweedie <- deviance(glm(claim_freq ~ ., data = freq_test, family = tweedie(var.power = optimal_var_power, link = "log")))
vars.to.remove <- c("exposure_id", "pet_gender", "pet_de_sexed_age", "pet_is_switcher", "nb_address_type_adj", "nb_suburb", "nb_state", "person_dob", "owner_age_years", "nb_breed_type",
"nb_breed_trait", "nb_breed_name_unique", "nb_breed_name_unique_concat", "exposure_id_1", "earned_units", "Total_Earned", "claim_nb", "Total_claim_amount",
"Total_claim_paid", "severity", "frequency", "is_multi_plan", "quote_time_group", "sa2_code", "nb_postcode", "is_multi_pet_plan", "pet_age_year")
frequency.model.data <- combined_data[,-which(colnames(combined_data) %in% vars.to.remove)]
frequency.model.data$qi <- as.factor(frequency.model.data$qi)
frequency.model.data <- na.omit(frequency.model.data)
str(frequency.model.data)
#Splitting into test and training
set.seed(2131)
freq_training_val_index <- sample(1:nrow(frequency.model.data), 0.7*nrow(frequency.model.data))
freq_training_val <- frequency.model.data[freq_training_val_index, ]
freq_test <- frequency.model.data[-freq_training_val_index, ]
var_powers <- seq(1, 2, by = 0.1)
# Initialize a vector to store the deviance results
deviance_results <- numeric(length(var_powers))
# Set up cross-validation control
train_control <- trainControl(method = "cv", number = 5, returnResamp = "final")
# Loop through each var.power value
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
# Train the Tweedie model with the current var.power
model <- train(
claim_freq ~ .,
data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"  # We still use RMSE to evaluate, but we will store deviance below
)
# Calculate the deviance for the model
# The fitted model can be extracted from train() results
# Note: The fitted model may not be directly accessible. Therefore, use the following approach.
# Manually calculate the deviance using the model
fitted_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = current_power, link = "log"))
# Store the deviance
deviance_results[i] <- deviance(fitted_model)
}
# Identify the optimal var.power value that minimizes deviance
optimal_var_power <- var_powers[which.min(deviance_results)]
cat("Optimal var.power:", optimal_var_power, "\n")
# Fit the final Tweedie model using the optimal var.power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
# Summarize the final model
summary(tweedie_freq_model)
# Performance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model, newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
# Calculate training deviance
training_deviance_tweedie <- deviance(tweedie_freq_model)
# Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model, newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
# Calculate test deviance
test_deviance_tweedie <- deviance(glm(claim_freq ~ ., data = freq_test, family = tweedie(var.power = optimal_var_power, link = "log")))
min(freq_training_val$claim_freq)
set.seed(12313)
alpha_values <- seq(0, 1, by = 0.1)
results <- data.frame(alpha = numeric(), lambda.min = numeric(), mse = numeric())
for (alpha in alpha_values) {
cv_fit <- cv.glmnet(X.training, Y.training, alpha = alpha, nfolds = 10)
results <- rbind(results, data.frame(alpha = alpha,
lambda.min = cv_fit$lambda.min,
mse = min(cv_fit$cvm)))
}
best_alpha <- results[which.min(results$mse), "alpha"]
best_lambda <- results[which.min(results$mse), "lambda.min"]
final_elastic_net_model <- glmnet(X.training, Y.training, alpha = best_alpha, lambda = best_lambda)
coef(final_elastic_net_model)
summary(final_elastic_net_model)
final_elastic_net_model_training_predictions <- predict(final_elastic_net_model, newx = X.training, type = "response")
final_elastic_net_model_training_predictions <- as.vector(final_elastic_net_model_training_predictions)
elastic_training_MSE <- mean((final_elastic_net_model_training_predictions-Y.training)^2)
elastic_net_cve <- min(results$mse)
##test performance
elastic_freq_predictions <- predict(final_elastic_net_model, newx = x.test, type = "response")
elastic_freq_predictions <- as.vector(elastic_freq_predictions)
elastic_test_mse <- mean((elastic_freq_predictions - y.test)^2)
#Assessing Distribution of claim_freq
mean_claim <- mean(frequency.model.data$claim_freq, na.rm = TRUE)
sd_claim <- sd(frequency.model.data$claim_freq, na.rm = TRUE)
ggplot(frequency.model.data, aes(x = claim_freq)) +
geom_density(fill = "lightblue", color = "darkblue", alpha = 0.6) +
stat_function(fun = dnorm, args = list(mean = mean_claim, sd = sd_claim),
color = "red", linetype = "dashed", size = 1) +
labs(title = "Density Plot of Claim Frequency with Normal Curve",
x = "Claim Frequency",
y = "Density") +
theme_minimal() +
xlim(0, 1)
###tweedie GLM####
var_powers <- seq(1, 2, by = 0.1)
mse_results <- numeric(length(var_powers))
train_control <- trainControl(method = "cv", number = 5)
for (i in seq_along(var_powers)) {
current_power <- var_powers[i]
model <- train(
claim_freq ~ ., data = freq_training_val,
method = "glm",
family = tweedie(var.power = current_power, link = "log"),
trControl = train_control,
metric = "RMSE"
)
mse_results[i] <- mean(model$resample$RMSE^2)
}
optimal_var_power <- var_powers[which.min(mse_results)]
optimal_var_power
tweedie_freq_model <- glm(claim_freq ~ ., data = freq_training_val, family = tweedie(var.power = optimal_var_power, link = "log"))
summary(tweedie_freq_model)
#Perfomance on the training data
tweedie_freq_model_training_predictions <- predict(tweedie_freq_model,  newdata = freq_training_val, type = "response")
tweedie_freq_model_training_predictions <- as.vector(tweedie_freq_model_training_predictions)
training_MSE_tweedie <- mean((tweedie_freq_model_training_predictions-freq_training_val$claim_freq)^2)
#Performance on the test data
tweedie_freq_model_prediction <- predict(tweedie_freq_model,  newdata = freq_test, type = "response")
tweedie_freq_model_prediction <- as.vector(tweedie_freq_model_prediction)
test_MSE_tweedie <- mean((tweedie_freq_model_prediction-freq_test$claim_freq)^2)
summary(tweedie_freq_model)
test_MSE_tweedie
logLik(tweedie_freq_model)
load("C:/Users/User/OneDrive/Documents/GitHub/ACTL4305/.RData")
summary(tweedie_freq_model)
sdu <- summary(tweedie_freq_model)
SST <- sum((freq_training_val$claim_freq - mean(freq_training_val$claim_freq))^2)
# Calculate residual sum of squares (RSS)
RSS <- sum((freq_training_val$claim_freq - predictions)^2)
# Calculate R-squared
R2 <- 1 - (RSS / SST)
# Calculate number of observations and predictors
n <- nrow(freq_training_val)
k <- length(coef(tweedie_freq_model)) - 1  # Subtracting 1 for the intercept
# Calculate adjusted R-squared
R2_adj <- 1 - ((1 - R2) * (n - 1) / (n - k - 1))
R2_adj
RSS <- sum((freq_training_val$claim_freq - predictions)^2)
R2_adj
test_MSE_tweedie
sqrt(test_MSE_tweedie)
min(mse_results)
training_MSE_tweedie
test_MSE_tweedie
tweedie_freq_model$fitted.values
residuals <- freq_training_val$claim_freq - tweedie_freq_model_training_predictions
# Step 2: Create a data frame for plotting
residuals_df <- data.frame(
Actual = freq_training_val$claim_freq,
Residuals = residuals
)
# Using ggplot2 for better visualization (make sure to load the library)
library(ggplot2)
# Step 3: Create the residuals vs. actual values plot
ggplot(residuals_df, aes(x = Actual, y = Residuals)) +
geom_point(alpha = 0.5) +            # Add points with some transparency
geom_hline(yintercept = 0, linetype = "dashed", color = "red") + # Add a horizontal line at 0
labs(
title = "Residuals vs. Actual Values",
x = "Actual Values",
y = "Residuals"
) +
theme_minimal()
fitted_values <- predict(tweedie_freq_model)
fitted_values
fitted_values <- predict(tweedie_freq_model)
# Calculate residuals
residuals <- freq_training_val$claim_freq - fitted_values
ggplot(data.frame(Fitted = fitted_values, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
theme_minimal()
fitted_values <- predict(tweedie_freq_model)
fitted_values
tweedie_freq_model$fitted.values
fitted_values <- tweedie_freq_model$fitted.values
# Calculate residuals
residuals <- freq_training_val$claim_freq - fitted_values
ggplot(data.frame(Fitted = fitted_values, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
theme_minimal()
fitted_values <- tweedie_freq_model$fitted.values
# Calculate residuals
residuals <- tweedie_freq_model$residuals
ggplot(data.frame(Fitted = fitted_values, Residuals = residuals), aes(x = Fitted, y = Residuals)) +
geom_point(alpha = 0.5) +
geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
labs(title = "Residuals vs Fitted Values", x = "Fitted Values", y = "Residuals") +
theme_minimal()
tweedie_freq_model$residuals
gplot(data.frame(Actual = freq_training_val$claim_freq, Predicted = fitted_values), aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual vs Predicted Values", x = "Actual Values", y = "Predicted Values") +
theme_minimal()
ggplot(data.frame(Actual = freq_training_val$claim_freq, Predicted = fitted_values), aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual vs Predicted Values", x = "Actual Values", y = "Predicted Values") +
theme_minimal()
ggplot(data.frame(Actual = freq_training_val$claim_freq, Predicted = fitted_values), aes(x = freq_training_val$claim_freq, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual vs Predicted Values", x = "Actual Values", y = "Predicted Values") +
theme_minimal()
ggplot(data.frame(Actual = freq_training_val$claim_freq, Predicted = fitted_values), aes(x = freq_training_val$claim_freq, y = Predicted)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual vs Predicted Values", x = "Actual Values", y = "Predicted Values") +
theme_minimal()
ggplot(data.frame(Actual = freq_training_val$claim_freq, residuals = tweedie_freq_model$residuals), aes(x = freq_training_val$claim_freq, y = residuals)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual vs Fitted Values", x = "Actual Values", y = "Predicted Values") +
theme_minimal()
ggplot(data.frame(Actual = freq_training_val$claim_freq, residuals = tweedie_freq_model$residuals), aes(x = freq_training_val$claim_freq, y = residuals)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual Frequency vs Residuals", x = "Actual Values", y = "Residuals") +
theme_minimal()
tweedie_freq_model$deviance
summary(tweedie_freq_model)
ggplot(data.frame(Actual = freq_training_val$claim_freq, residuals = tweedie_freq_model$residuals), aes(x = freq_training_val$claim_freq, y = residuals)) +
geom_point(alpha = 0.5) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
labs(title = "Actual Frequency vs Residuals", x = "Actual Values", y = "Residuals") +
theme_minimal() +
xlim(c(0,1))
load("C:/Users/User/OneDrive/Documents/GitHub/ACTL4305/.RData")
load("C:/Users/User/OneDrive/Documents/GitHub/ACTL4305/.RData")
load("C:/Users/User/OneDrive/Documents/GitHub/ACTL4305/.RData")
